{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.8.8\n",
      "OpenCV version:  4.0.1\n",
      "Numpy version:  1.20.1\n",
      "Tensorflow version:  2.3.0\n",
      "Pickle version:  4.0\n",
      "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n",
      "Streaming video using device...\n",
      "\n",
      "Loading HAAR classifiers...\n",
      "\n",
      "Loading model...\n",
      "\n",
      "['conv_0', 'bn_0', 'relu_0', 'conv_1', 'bn_1', 'relu_1', 'conv_2', 'bn_2', 'relu_2', 'conv_3', 'bn_3', 'relu_3', 'shortcut_4', 'conv_5', 'bn_5', 'relu_5', 'conv_6', 'bn_6', 'relu_6', 'conv_7', 'bn_7', 'relu_7', 'shortcut_8', 'conv_9', 'bn_9', 'relu_9', 'conv_10', 'bn_10', 'relu_10', 'shortcut_11', 'conv_12', 'bn_12', 'relu_12', 'conv_13', 'bn_13', 'relu_13', 'conv_14', 'bn_14', 'relu_14', 'shortcut_15', 'conv_16', 'bn_16', 'relu_16', 'conv_17', 'bn_17', 'relu_17', 'shortcut_18', 'conv_19', 'bn_19', 'relu_19', 'conv_20', 'bn_20', 'relu_20', 'shortcut_21', 'conv_22', 'bn_22', 'relu_22', 'conv_23', 'bn_23', 'relu_23', 'shortcut_24', 'conv_25', 'bn_25', 'relu_25', 'conv_26', 'bn_26', 'relu_26', 'shortcut_27', 'conv_28', 'bn_28', 'relu_28', 'conv_29', 'bn_29', 'relu_29', 'shortcut_30', 'conv_31', 'bn_31', 'relu_31', 'conv_32', 'bn_32', 'relu_32', 'shortcut_33', 'conv_34', 'bn_34', 'relu_34', 'conv_35', 'bn_35', 'relu_35', 'shortcut_36', 'conv_37', 'bn_37', 'relu_37', 'conv_38', 'bn_38', 'relu_38', 'conv_39', 'bn_39', 'relu_39', 'shortcut_40', 'conv_41', 'bn_41', 'relu_41', 'conv_42', 'bn_42', 'relu_42', 'shortcut_43', 'conv_44', 'bn_44', 'relu_44', 'conv_45', 'bn_45', 'relu_45', 'shortcut_46', 'conv_47', 'bn_47', 'relu_47', 'conv_48', 'bn_48', 'relu_48', 'shortcut_49', 'conv_50', 'bn_50', 'relu_50', 'conv_51', 'bn_51', 'relu_51', 'shortcut_52', 'conv_53', 'bn_53', 'relu_53', 'conv_54', 'bn_54', 'relu_54', 'shortcut_55', 'conv_56', 'bn_56', 'relu_56', 'conv_57', 'bn_57', 'relu_57', 'shortcut_58', 'conv_59', 'bn_59', 'relu_59', 'conv_60', 'bn_60', 'relu_60', 'shortcut_61', 'conv_62', 'bn_62', 'relu_62', 'conv_63', 'bn_63', 'relu_63', 'conv_64', 'bn_64', 'relu_64', 'shortcut_65', 'conv_66', 'bn_66', 'relu_66', 'conv_67', 'bn_67', 'relu_67', 'shortcut_68', 'conv_69', 'bn_69', 'relu_69', 'conv_70', 'bn_70', 'relu_70', 'shortcut_71', 'conv_72', 'bn_72', 'relu_72', 'conv_73', 'bn_73', 'relu_73', 'shortcut_74', 'conv_75', 'bn_75', 'relu_75', 'conv_76', 'bn_76', 'relu_76', 'conv_77', 'bn_77', 'relu_77', 'conv_78', 'bn_78', 'relu_78', 'conv_79', 'bn_79', 'relu_79', 'conv_80', 'bn_80', 'relu_80', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'relu_84', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'relu_87', 'conv_88', 'bn_88', 'relu_88', 'conv_89', 'bn_89', 'relu_89', 'conv_90', 'bn_90', 'relu_90', 'conv_91', 'bn_91', 'relu_91', 'conv_92', 'bn_92', 'relu_92', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'relu_96', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'relu_99', 'conv_100', 'bn_100', 'relu_100', 'conv_101', 'bn_101', 'relu_101', 'conv_102', 'bn_102', 'relu_102', 'conv_103', 'bn_103', 'relu_103', 'conv_104', 'bn_104', 'relu_104', 'conv_105', 'permute_106', 'yolo_106']\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n",
      "Started training model for Komal\n",
      "Model trained successfully for Komal\n",
      "Started training model for Ananya\n",
      "Model trained successfully for Ananya\n",
      "Started training model for Arunima\n",
      "Model trained successfully for Arunima\n",
      "car: 99.17%\n",
      "Distance:2.76 feet\n",
      "Distance: 0.0906 ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-65c3a9235e04>:82: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces == ():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car: 99.16%\n",
      "Distance:2.76 feet\n",
      "Distance: 0.0906 ft\n",
      "car: 99.80%\n",
      "Distance:3.01 feet\n",
      "Distance: 0.0988 ft\n",
      "car: 99.85%\n",
      "Distance:3.08 feet\n",
      "Distance: 0.101 ft\n",
      "car: 99.97%\n",
      "Distance:3.61 feet\n",
      "Distance: 0.1184 ft\n",
      "Go (Green light): 94.7542\n",
      "car: 99.89%\n",
      "Distance:3.71 feet\n",
      "Distance: 0.1217 ft\n",
      "Go (Green light): 94.3464\n",
      "car: 99.95%\n",
      "Distance:4.04 feet\n",
      "Distance: 0.1325 ft\n",
      "Go (Green light): 96.8726\n",
      "Go (Green light): 93.7406\n",
      "Go (Green light): 97.1092\n",
      "car: 97.29%\n",
      "Distance:3.71 feet\n",
      "Distance: 0.1217 ft\n",
      "Go (Green light): 90.2465\n",
      "Go (Green light): 97.4098\n",
      "car: 99.76%\n",
      "Distance:3.98 feet\n",
      "Distance: 0.1306 ft\n",
      "car: 99.81%\n",
      "Distance:3.98 feet\n",
      "Distance: 0.1306 ft\n",
      "Go (Green light): 96.6471\n",
      "Go (Green light): 94.4697\n",
      "Go (Green light): 92.9865\n",
      "car: 93.29%\n",
      "Distance:4.67 feet\n",
      "Distance:6.94 feet\n",
      "Distance: 0.1532 ft\n",
      "Distance: 0.2277 ft\n",
      "End of speed limit (80km/h): 94.8257\n",
      "Go (Green light): 98.0411\n",
      "Go (Green light): 90.5271\n",
      "Distance:3.87 feet\n",
      "Distance: 0.127 ft\n",
      "Go (Green light): 97.8973\n",
      "Go (Green light): 96.1825\n",
      "Go (Green light): 96.9986\n",
      "Go (Green light): 98.2502\n",
      "Distance:6.29 feet\n",
      "Distance: 0.2064 ft\n",
      "Elapsed time: 25.69\n",
      "Approximate FPS: 0.47\n",
      "pacemood h 1\n",
      " ' ' tterl§€uieed}I7)l0nely\n",
      "\n",
      "o\n",
      "\n",
      "tranqullhty . Co a e\n",
      "amOngS. lltgllge/'1c’i1iEtressn g\n",
      "\n",
      "g1‘0uI.1d “V9? meadgvnv\n",
      "mot1onS13nd1n§feeding\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import cv2\n",
    "from math import pow, sqrt\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from numpy.core.records import array\n",
    "from platform import python_version\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "print(\"Python version: \", python_version())\n",
    "print(\"OpenCV version: \", cv2.__version__)\n",
    "print(\"Numpy version: \", np.version.version)\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Pickle version: \", pickle.format_version)\n",
    "print(sys.version)\n",
    "\n",
    "# Parse the arguments from command line\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-v', '--video', type = str, default = 'traffic_videos/foggymorning.mp4', help = 'Video file path. If no path is given, video is captured using device.')\n",
    "\n",
    "parser.add_argument('-m', '--model', default = 'SSD_MobileNet.caffemodel', help = \"Path to the pretrained model.\")\n",
    "    \n",
    "parser.add_argument('-p', '--prototxt', default = 'SSD_MobileNet_prototxt.txt', help = 'Prototxt of the model.')\n",
    "\n",
    "parser.add_argument('-l', '--labels', default = 'class_labels.txt', help = 'Labels of the dataset.')\n",
    "\n",
    "parser.add_argument('-y', '--cfg', default = 'yolov3.cfg', help = 'Path_to_yolo_caffemodel')\n",
    "\n",
    "parser.add_argument('-w', '--weights', default = 'yolov3.weights', help = 'Prototxt file for yolo')\n",
    "\n",
    "parser.add_argument('-x', '--excel', default = 'label_names.txt', help = 'Text file for Traffic_Sign_Detection')\n",
    "\n",
    "parser.add_argument('-c', '--confidence', type = float, default = 0.9, help='Set confidence for detecting objects')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "# Loading mean image to use for preprocessing further; Opening file for reading in binary mode\n",
    "with open('mean_image_rgb.pickle', 'rb') as f:\n",
    "    mean = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "\n",
    "labels = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\"diningtable\",\n",
    "            \"dog\",\"horse\", \"motorbike\",\"person\", \"pottedplant\", \"sheep\",\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(labels), 3))\n",
    "# Read the csv file for traffic-sign and print first five records\n",
    "tf_labels = [\"Traffic light\",\"Speed limit (30km/h)\",\"Speed limit (50km/h)\",\"End of speed limit (80km/h)\",\n",
    "             \"Ready (Yellow light)\",\"Go (Green light)\",\"Stop (Red light)\", \"Speed limit (100km/h)\",\"Pedestrians\",\"No passing\",\n",
    "             \"No passing for vehicles over 3.5 metric tons\",\"Right-of-way at the next intersection\",\"Priority road\",\n",
    "             \"Yield\",\"Men at work\",\"No vehicles\",\"Vehicles over 3.5 metric tons prohibited\",\"No entry\",\"General caution\",\n",
    "             \"Dangerous curve to the left\",\"Dangerous curve to the right\",\"Double curve\",\"Bumpy road\",\"Slippery road\",\n",
    "             \"Road narrows on the right\",\"Road work\",\"Traffic signals\",\"Footpath\",\"Children crossing\",\"Bicycles crossing\",\n",
    "             \"Beware of ice/snow\",\"Wild animals crossing\",\"End of all speed and passing limits\",\"Turn right ahead\",\"Turn left ahead\",\n",
    "             \"Ahead only\",\"Go straight or right\",\"Go straight or left\",\"Keep right\",\"Keep left\",\"Roundabout mandatory\",\n",
    "             \"End of no passing\",\"End of no passing by vehicles over 3.5 metric tons\"]\n",
    "print(\"Streaming video using device...\\n\")\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_frontalface_default.xml')\n",
    "profile_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_profileface.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_features/haarcascade_eye.xml')\n",
    "print(\"Loading HAAR classifiers...\\n\")\n",
    "\n",
    "\n",
    "# Function to detect face\n",
    "def face_detector(img, size=0.5):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale( gray, 1.3, 5, minSize = (30,30))\n",
    "    # If face not found return blank region\n",
    "    if faces == ():\n",
    "        return [img, [], None]\n",
    "    # Obtain Region of face\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))        \n",
    "        profile = profile_classifier.detectMultiScale(img, 1.3,5)\n",
    "        for (px,py,pw,ph) in profile:\n",
    "            cv2.rectangle(img,(px,py),(px+pw,py+ph), (0,255,255),2)         \n",
    "        eyes = eye_classifier.detectMultiScale(img, 1.3,4)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh), (0,255,255),2) \n",
    "    return [img, roi, faces[0]]   \n",
    "\n",
    "# Capture video from file or through webcam\n",
    "if args.video:\n",
    "    cap = cv2.VideoCapture(args.video)    \n",
    "else:\n",
    "    cap = cv2.VideoCapture(0)    \n",
    "#initialize the FPS counter\n",
    "fps = FPS().start()\n",
    "#Load the Caffe model \n",
    "print(\"Loading model...\\n\")\n",
    "net = cv2.dnn.readNetFromCaffe(args.prototxt, args.model)\n",
    "d_net = cv2.dnn.readNetFromDarknet(args.cfg, args.weights)\n",
    "\n",
    "# To use with GPU\n",
    "d_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "d_net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n",
    "# Getting names of all YOLO v3 layers\n",
    "layers_all = d_net.getLayerNames()\n",
    "print(layers_all)\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i[0] - 1] for i in d_net.getUnconnectedOutLayers()]\n",
    "print(layers_names_output)\n",
    "\n",
    "# Facial Recognition model training \n",
    "models = {\"Komal\": {\"data_path\": \"face/komal/\",\"files\": [],\"model\": None},\n",
    "          \"Ananya\": {\"data_path\": \"face/ananya/\",\"files\": [],\"model\": None},\n",
    "          \"Arunima\": {\"data_path\": \"face/arunima/\",\"files\": [],\"model\": None}\n",
    "         }\n",
    "for key in models:\n",
    "    print(\"Started training model for \" + key)\n",
    "    models[key][\"files\"] = [f for f in listdir(models[key][\"data_path\"]) if isfile(join(models[key][\"data_path\"], f))]\n",
    "    Training_Data, Labels = [], []\n",
    "\n",
    "    for i, files in enumerate(models[key][\"files\"]):\n",
    "        image_path = models[key][\"data_path\"] + models[key][\"files\"][i]\n",
    "        images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        Training_Data.append( np.asarray( images, dtype=np.uint8))\n",
    "        Labels.append(i)\n",
    "\n",
    "    # Create a numpy array for both training data and labels\n",
    "    Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "    # Initialize facial recognizer\n",
    "    models[key][\"model\"] =  cv2.face.LBPHFaceRecognizer_create()\n",
    "    # NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "    # Let's train our model\n",
    "    models[key][\"model\"].train(np.asarray(Training_Data), np.asarray(Labels))\n",
    "    print(\"Model trained successfully for \" + key)\n",
    "\n",
    "while True:  \n",
    "    ret, frame = cap.read()\n",
    "    ar = face_detector(frame)\n",
    "    face=ar[1] \n",
    "    pos=ar[2]\n",
    "    time.sleep(0.06)\n",
    "    if not ret:\n",
    "        break   \n",
    "\n",
    "    # grab the frame from the threaded video stream and resize it to have a maximum width of 600 pixels    \n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    # grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),0.007843, (300, 300), 127.5)\n",
    "    # Blob from current frame of traffic sign video\n",
    "    tf_blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    # pass the blob through the network and obtain the detections and predictions\n",
    "    net.setInput(blob)\n",
    "    d_net.setInput(tf_blob)\n",
    "    detections = net.forward()\n",
    "    tf_detections = d_net.forward(layers_names_output)\n",
    "       \n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is greater than the minimum confidence\n",
    "        if confidence > args.confidence:\n",
    "            # extract the index of the class label from the`detections`, then compute the (x, y)coordinates of the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")            \n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(labels[idx],confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),COLORS[idx], 1)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startY, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 1)  \n",
    "            print(label)\n",
    "    \n",
    "    \n",
    "    pos_dict = dict()\n",
    "    coordinates = dict()\n",
    "    # Focal length (in cm)\n",
    "    F = 50  \n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")  \n",
    "        coordinates[i] = (startX, startY, endX, endY)\n",
    "        # Mid point of bounding box\n",
    "        x_mid = round((startX+endX)/2,4)\n",
    "        y_mid = round((startY+endY)/2,4)\n",
    "        height = round(endY-startY,4)\n",
    "\n",
    "        # Distance from camera based on triangle similarity\n",
    "        distance = round(((165 * F)/height)/30.48,2)\n",
    "        print(\"Distance:{dist}\".format(dist = distance), \"feet\")\n",
    "\n",
    "        # Mid-point of bounding boxes (in cm) based on triangle similarity technique\n",
    "        x_mid_cm = (x_mid * distance) / F\n",
    "        y_mid_cm = (y_mid * distance) / F\n",
    "        pos_dict[i] = (x_mid,y_mid,distance)\n",
    "    \n",
    "    # Distance between every object detected in a frame\n",
    "    close_objects = set()\n",
    "    for i in pos_dict.keys():\n",
    "        for j in pos_dict.keys():\n",
    "            if i < j:\n",
    "                dist = sqrt(pow(pos_dict[i][0]-pos_dict[j][0],2) + pow(pos_dict[i][1]-pos_dict[j][1],2) + pow(pos_dict[i][2]-pos_dict[j][2],2))\n",
    "\n",
    "                # Check if distance less than 1 feet (300 mm approx):\n",
    "                if dist < 30:\n",
    "                    close_objects.add(i)\n",
    "                    close_objects.add(j)\n",
    "    for i in pos_dict.keys():\n",
    "        if i in close_objects:\n",
    "            COLOR = (0,0,255)\n",
    "        else:\n",
    "            COLOR = (0,255,0)     \n",
    "        (startX, startY, endX, endY) = coordinates[i]\n",
    "        cv2.rectangle(frame,(startX,startY), (endX, endY), COLOR, 1)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15        \n",
    "        # Convert mms to feet\n",
    "        dist_between_obj = \"Distance: {i} ft\".format(i=round(pos_dict[i][2]/30.48,4))\n",
    "        cv2.putText(frame, dist_between_obj , (y, startY),cv2.FONT_HERSHEY_SIMPLEX, 0.45, COLOR, 1)\n",
    "        print(dist_between_obj)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)    \n",
    "    \n",
    "    try:\n",
    "        traffic_found = False\n",
    "        tf_label = traffic_found\n",
    "        # Lists for class's number    \n",
    "        class_numbers = []\n",
    "        # Going through all output layers after feed forward pass\n",
    "        for traffic_result in tf_detections:\n",
    "            # Going through all detections from current output layer\n",
    "            for detected_objects in traffic_result:\n",
    "                \n",
    "                # Getting 43 classes' probabilities for current detected object\n",
    "                scores = detected_objects[9:]\n",
    "                # Getting index of the class with the maximum value of probability\n",
    "                class_current = np.argmax(scores)\n",
    "                # Getting value of probability for defined class\n",
    "                confidence_current = scores[class_current]\n",
    "                # Minimum probability to eliminate weak detections\n",
    "                probability_minimum = 0.9\n",
    "                # Setting threshold to filtering weak bounding boxes by non-maximum suppression     \n",
    "                blob_ts = cv2.dnn.blobFromImage(frame, 1 / 255.0, size = (32,32), swapRB=True, crop=False)\n",
    "                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "                prediction = np.argmax(scores)\n",
    "                class_numbers.append(class_current)\n",
    "\n",
    "                # Eliminating weak predictions by minimum probability\n",
    "                if confidence_current > probability_minimum:\n",
    "                    traffic_found == True\n",
    "                    # Scaling bounding box coordinates to the initial frame size\n",
    "                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "                    (startX, startY, endX, endY) = box_current.astype(\"int\")  \n",
    "                    # draw the prediction on the frame\n",
    "                    tf_label = '{}: {:.4f}'.format(tf_labels[prediction],confidence_current*100)\n",
    "                    cv2.rectangle(frame, (startX, startY), (endX, endY),(0,0,225), 1)\n",
    "                    y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "                    cv2.putText(frame, \"Traffic detected \" + tf_label, (startY, y),cv2.FONT_HERSHEY_SIMPLEX,0.4, (0,0,200), 1)  \n",
    "                    print(tf_label)\n",
    "    except Exception as e:\n",
    "        cv2.putText(frame, \"No traffic detected\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,155,255), 1)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "        pass\n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        foundFace = False\n",
    "        user = None\n",
    "        confidence = 85\n",
    "        for key in models:\n",
    "            if foundFace == True:\n",
    "                break\n",
    "            results = models[key][\"model\"].predict(face)\n",
    "            if results[1] < 500:\n",
    "                confidence = int( 100 * (1 - (results[1])/500) )\n",
    "                if confidence > 85:\n",
    "                    user = key\n",
    "                    foundFace = True        \n",
    "        posX = pos[0] + 5\n",
    "        posY = pos[0] - 5\n",
    "        cv2.putText(frame, \"Face Detected \" + str(confidence) + \"%\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "        if foundFace == True:\n",
    "            cv2.putText(frame, user, (posX, posY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)   \n",
    "            print(user)\n",
    "            print(\"Face Detected \" + str(confidence) + \"%\")            \n",
    "        else:\n",
    "            cv2.putText(frame, \"Unknown \", (posX, posY), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,153,255), 2)\n",
    "\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "    # Raise exception in case, no image is found\n",
    "    except Exception as e:\n",
    "        cv2.putText(frame, \"Accuracy 0% (No face detected)\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,155,255), 1)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "        pass\n",
    "    \n",
    "    # Show the output frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.resizeWindow('Frame',1200,1000)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    #update the FPS counter\n",
    "    fps.update()  \n",
    "#stop the timer and display FPS count \n",
    "fps.stop()\n",
    "print(\"Elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approximate FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "import glob\n",
    "for img in glob.glob(\"*.jpg\"):\n",
    "    cv_img = cv2.imread(img)\n",
    "demo = Image.fromarray(cv_img)\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract.exe'\n",
    "text_reader = pytesseract.image_to_string(demo, lang = 'eng')\n",
    "print(text_reader)    \n",
    "# Clean\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "reader = label + \"Traffic detected\" + str(tf_label) + dist_between_obj +\"Face Detected \" + str(confidence) + \"%\" + str(user) +\"Distance:{dist}\".format(dist = distance)+\"feet\"\n",
    "print(str(reader))\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(str(cap)) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3 \n",
    "engine = pyttsx3.init('sapi5')\n",
    "voices = engine.getProperty('voices')\n",
    "for voice in voices:\n",
    "    engine.setProperty('voice', voices[0].id)\n",
    "rate = engine.getProperty('rate')\n",
    "engine.setProperty('rate', 160)\n",
    "engine.say(reader)\n",
    "engine.save_to_file(str(reader), \"demo.mp3\")\n",
    "engine.runAndWait()\n",
    "engine.stop()\n",
    "\n",
    "from gtts import gTTS\n",
    "tts = gTTS(str(reader), lang='en')\n",
    "tts.save('audio1.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
